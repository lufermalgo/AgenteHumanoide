import React, { useState, useEffect, useRef } from 'react';
import styled from 'styled-components';
import { theme } from '../../styles/theme';
import { GeminiLiveService, ASSESSMENT_GEMINI_CONFIG } from '../../services/geminiLive';

const InterfaceContainer = styled.div`
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.lg};
  padding: ${theme.spacing.lg};
  background: ${theme.colors.background};
  border-radius: ${theme.borderRadius.lg};
  max-width: 600px;
  margin: 0 auto;
`;

const Section = styled.div`
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.md};
`;

const SectionTitle = styled.h3`
  font-size: ${theme.typography.fontSizes.lg};
  font-weight: ${theme.typography.fontWeights.semibold};
  color: ${theme.colors.text};
  margin: 0;
  display: flex;
  align-items: center;
  gap: ${theme.spacing.sm};
`;

const QuestionText = styled.div`
  font-size: ${theme.typography.fontSizes.lg};
  color: ${theme.colors.text};
  line-height: 1.6;
  padding: ${theme.spacing.lg};
  background: ${theme.colors.backgroundCard};
  border-radius: ${theme.borderRadius.md};
  border-left: 4px solid ${theme.colors.primary};
`;

const ConversationArea = styled.div`
  min-height: 200px;
  padding: ${theme.spacing.lg};
  background: ${theme.colors.backgroundCard};
  border-radius: ${theme.borderRadius.md};
  border: 2px solid ${theme.colors.lightGray};
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.md};
`;

const ConversationBubble = styled.div.withConfig({
  shouldForwardProp: (prop) => !['isUser'].includes(prop)
})<{ isUser: boolean }>`
  padding: ${theme.spacing.md};
  border-radius: ${theme.borderRadius.md};
  max-width: 80%;
  align-self: ${props => props.isUser ? 'flex-end' : 'flex-start'};
  background: ${props => props.isUser ? theme.colors.primary : theme.colors.lightGray};
  color: ${props => props.isUser ? 'white' : theme.colors.text};
  font-size: ${theme.typography.fontSizes.md};
  line-height: 1.4;
  
  &::before {
    content: '${props => props.isUser ? 'ğŸ‘¤' : 'ğŸ¤–'}';
    margin-right: ${theme.spacing.sm};
  }
`;

const VoiceControls = styled.div`
  display: flex;
  justify-content: center;
  gap: ${theme.spacing.md};
  align-items: center;
`;



const StatusIndicator = styled.div<{ status: 'idle' | 'listening' | 'processing' | 'speaking' | 'error' }>`
  display: flex;
  align-items: center;
  gap: ${theme.spacing.sm};
  padding: ${theme.spacing.sm} ${theme.spacing.md};
  border-radius: ${theme.borderRadius.md};
  font-size: ${theme.typography.fontSizes.sm};
  font-weight: ${theme.typography.fontWeights.medium};
  
  background: ${props => {
    switch (props.status) {
      case 'listening': return '#2196F3';
      case 'processing': return '#FF9800';
      case 'speaking': return '#4CAF50';
      case 'error': return '#F44336';
      default: return theme.colors.lightGray;
    }
  }};
  
  color: ${props => props.status === 'idle' ? theme.colors.text : 'white'};
`;

const ActionButtons = styled.div`
  display: flex;
  gap: ${theme.spacing.md};
  justify-content: center;
  flex-wrap: wrap;
`;

const ActionButton = styled.button<{ variant?: 'primary' | 'secondary' | 'success' }>`
  padding: ${theme.spacing.md} ${theme.spacing.lg};
  border: none;
  border-radius: ${theme.borderRadius.md};
  font-size: ${theme.typography.fontSizes.md};
  font-weight: ${theme.typography.fontWeights.medium};
  cursor: pointer;
  transition: all 0.3s ease;
  
  background: ${props => {
    switch (props.variant) {
      case 'success': return '#4CAF50';
      case 'secondary': return theme.colors.lightGray;
      default: return theme.colors.primary;
    }
  }};
  
  color: ${props => props.variant === 'secondary' ? theme.colors.text : 'white'};

  &:hover {
    transform: translateY(-1px);
    background: ${props => {
      switch (props.variant) {
        case 'success': return '#45a049';
        case 'secondary': return theme.colors.tertiary;
        default: return theme.colors.secondary;
      }
    }};
  }

  &:disabled {
    opacity: 0.6;
    cursor: not-allowed;
    transform: none;
  }
`;

interface ConversationTurn {
  id: string;
  isUser: boolean;
  text: string;
  timestamp: Date;
  audioUrl?: string;
}

export interface GeminiVoiceInterfaceProps {
  question: string;
  onResponse: (response: string) => void;
  onNext?: () => void;
  onSkip?: () => void;
  autoReadQuestion?: boolean;
}

const GeminiVoiceInterface: React.FC<GeminiVoiceInterfaceProps> = ({
  question,
  onResponse,
  onNext,
  onSkip,
  autoReadQuestion = true
}) => {
  // Simplificar: no usar hook complejo, manejar stream directamente como Open WebUI
  const [stream, setStream] = useState<MediaStream | null>(null);
  
  const [status, setStatus] = useState<'idle' | 'listening' | 'processing' | 'speaking' | 'error'>('idle');
  const [conversation, setConversation] = useState<ConversationTurn[]>([]);
  const [isListening, setIsListening] = useState(false);
  const [geminiService, setGeminiService] = useState<GeminiLiveService | null>(null);
  const [hasReadQuestion, setHasReadQuestion] = useState(false);
  
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Inicializar Gemini Live API
  useEffect(() => {
    const initGemini = async () => {
      try {
        const service = new GeminiLiveService();
        
        // El servicio ahora funciona de forma sÃ­ncrona con la API estÃ¡ndar
        console.log('âœ… Servicio Gemini listo para transcripciÃ³n');
        
        await service.connect(ASSESSMENT_GEMINI_CONFIG);
        setGeminiService(service);
        console.log('âœ… Gemini conectado y listo para usar');
        
      } catch (error) {
        console.error('âŒ Error inicializando Gemini Live:', error);
        setStatus('error');
      }
    };

    // Inicializar Gemini cuando el componente se monta
    if (!geminiService) {
      initGemini();
    }
    
    return () => {
      geminiService?.disconnect();
    };
  }, [geminiService]);

  // Cargar voces disponibles
  useEffect(() => {
    const loadVoices = () => {
      const voices = speechSynthesis.getVoices();
      if (voices.length > 0) {
        console.log('ğŸ—£ï¸ Voces cargadas:', voices.length);
      }
    };
    
    // Las voces pueden cargarse de forma asÃ­ncrona
    if (speechSynthesis.getVoices().length === 0) {
      speechSynthesis.addEventListener('voiceschanged', loadVoices);
    } else {
      loadVoices();
    }
    
    return () => {
      speechSynthesis.removeEventListener('voiceschanged', loadVoices);
    };
  }, []);

  // Auto-leer pregunta
  useEffect(() => {
    if (autoReadQuestion && geminiService?.connected && question && !hasReadQuestion) {
      readQuestion();
      setHasReadQuestion(true);
    }
  }, [autoReadQuestion, geminiService?.connected, question, hasReadQuestion, readQuestion]);

  const addConversationTurn = (isUser: boolean, text: string, audioUrl?: string) => {
    const turn: ConversationTurn = {
      id: Date.now().toString(),
      isUser,
      text,
      timestamp: new Date(),
      audioUrl
    };
    
    setConversation(prev => [...prev, turn]);
    
    if (isUser) {
      onResponse(text);
    }
  };

  const readQuestion = async () => {
    if (!geminiService?.connected) return;
    
    try {
      setStatus('speaking');
      
      // Buscar una voz en espaÃ±ol mÃ¡s natural
      const voices = speechSynthesis.getVoices();
      console.log('ğŸ—£ï¸ Voces disponibles:', voices.map(v => `${v.name} (${v.lang})`));
      
      // Priorizar voces colombianas, luego espaÃ±olas, luego cualquier espaÃ±ol
      const preferredVoice = voices.find(voice => 
        voice.lang.includes('es-CO') || voice.lang.includes('es-MX') || voice.lang.includes('es-AR')
      ) || voices.find(voice => 
        voice.lang.includes('es-ES')
      ) || voices.find(voice => 
        voice.lang.startsWith('es')
      );
      
      const utterance = new SpeechSynthesisUtterance(question);
      utterance.lang = 'es-CO'; // Colombiano si estÃ¡ disponible
      utterance.rate = 0.85; // MÃ¡s lento para mejor comprensiÃ³n
      utterance.pitch = 1.0; // Tono natural
      utterance.volume = 0.8; // Volumen cÃ³modo
      
      if (preferredVoice) {
        utterance.voice = preferredVoice;
        console.log('ğŸ—£ï¸ Usando voz:', preferredVoice.name, preferredVoice.lang);
      }
      
      // Detectar interrupciones del usuario
      const checkForInterruption = () => {
        if (isListening) {
          console.log('ğŸ—£ï¸ Usuario interrumpiÃ³ al agente');
          speechSynthesis.cancel(); // Parar al agente
          setStatus('listening');
          return true;
        }
        return false;
      };
      
      // Verificar interrupciones cada 100ms
      const interruptionInterval = setInterval(() => {
        if (checkForInterruption()) {
          clearInterval(interruptionInterval);
        }
      }, 100);
      
      utterance.onend = () => {
        clearInterval(interruptionInterval);
        console.log('ğŸ—£ï¸ Agente terminÃ³ de hablar, iniciando escucha automÃ¡tica...');
        setStatus('idle');
        // Iniciar escucha automÃ¡tica despuÃ©s de que termine de hablar
        setTimeout(() => {
          console.log('â° Timeout ejecutado, iniciando escucha automÃ¡tica...');
          if (geminiService?.connected) {
            console.log('ğŸ™ï¸ Iniciando escucha automÃ¡tica...');
            startListening();
          } else {
            console.warn('âš ï¸ Gemini no conectado');
          }
        }, 500); // PequeÃ±a pausa para transiciÃ³n natural
      };
      utterance.onerror = (error) => {
        clearInterval(interruptionInterval);
        console.error('âŒ Error en sÃ­ntesis de voz:', error);
        setStatus('idle');
      };
      
      speechSynthesis.speak(utterance);
    } catch (error) {
      console.error('âŒ Error leyendo pregunta:', error);
      setStatus('idle');
    }
  };

  const startListening = async () => {
    console.log('ğŸ™ï¸ Iniciando escucha siguiendo patrÃ³n Open WebUI...');
    console.log('- Gemini conectado:', geminiService?.connected);
    if (!geminiService?.connected) {
      console.error('âŒ Gemini no conectado');
      return;
    }

    // Usar Web Speech API como Open WebUI
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
      try {
        setIsListening(true);
        setStatus('listening');
        
        // Crear SpeechRecognition object con tipos correctos
        const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
        const speechRecognition = new SpeechRecognition();
        
        // Configurar como Open WebUI
        speechRecognition.continuous = true;
        speechRecognition.interimResults = false;
        speechRecognition.lang = 'es-CO';
        
        let transcription = '';
        let timeoutId: NodeJS.Timeout;
        const inactivityTimeout = 3000; // 3 segundos de silencio
        
        speechRecognition.onstart = () => {
          console.log('ğŸ¤ Speech recognition iniciado');
        };
        
        speechRecognition.onresult = async (event: any) => {
          console.log('ğŸµ Speech reconocido:', event);
          
          // Limpiar timeout de inactividad
          clearTimeout(timeoutId);
          
          // Obtener transcripciÃ³n
          const transcript = event.results[Object.keys(event.results).length - 1][0].transcript;
          transcription = `${transcription}${transcript}`;
          
          console.log('âœ… TranscripciÃ³n en tiempo real:', transcription);
          
          // Reiniciar timeout de inactividad
          timeoutId = setTimeout(() => {
            console.log('â° Timeout de inactividad, procesando transcripciÃ³n final...');
            speechRecognition.stop();
          }, inactivityTimeout);
        };
        
        speechRecognition.onend = async () => {
          console.log('ğŸ¤ Speech recognition terminado, procesando transcripciÃ³n...');
          clearTimeout(timeoutId);
          
          if (transcription.trim()) {
            console.log('âœ… TranscripciÃ³n final:', transcription);
            
            // Agregar transcripciÃ³n a la conversaciÃ³n
            addConversationTurn(true, transcription);
            
            // Generar respuesta automÃ¡tica del agente
            try {
              console.log('ğŸ¤– Generando respuesta del agente...');
              const agentResponse = await geminiService.generateTextResponse(
                `El usuario respondiÃ³: "${transcription}". Genera una respuesta natural y empÃ¡tica que confirme que entendiste su respuesta y continÃºe la conversaciÃ³n de forma fluida. Responde de forma concisa.`
              );
              
              console.log('ğŸ¤– Respuesta del agente:', agentResponse);
              addConversationTurn(false, agentResponse);
              
              // Leer la respuesta del agente automÃ¡ticamente
              const utterance = new SpeechSynthesisUtterance(agentResponse);
              utterance.lang = 'es-CO';
              utterance.rate = 0.85;
              utterance.pitch = 1.0;
              utterance.volume = 0.8;
              
              utterance.onend = () => {
                console.log('ğŸ¤– Agente terminÃ³ de responder, iniciando siguiente ciclo...');
                setStatus('idle');
                // Iniciar escucha automÃ¡tica para la siguiente interacciÃ³n
                setTimeout(() => {
                  if (geminiService?.connected) {
                    startListening();
                  }
                }, 500);
              };
              
              setStatus('speaking');
              speechSynthesis.speak(utterance);
              
            } catch (error) {
              console.error('âŒ Error generando respuesta del agente:', error);
              setStatus('idle');
              setIsListening(false);
            }
          } else {
            console.log('âš ï¸ No se detectÃ³ transcripciÃ³n, reiniciando escucha...');
            setStatus('idle');
            setIsListening(false);
            // Reiniciar escucha automÃ¡ticamente
            setTimeout(() => {
              if (geminiService?.connected) {
                startListening();
              }
            }, 1000);
          }
        };
        
        speechRecognition.onerror = (event: any) => {
          console.error('âŒ Error en speech recognition:', event);
          setStatus('error');
          setIsListening(false);
        };
        
        // Iniciar recognition
        speechRecognition.start();
        
      } catch (error) {
        console.error('âŒ Error iniciando speech recognition:', error);
        setStatus('error');
        setIsListening(false);
      }
    } else {
      console.error('âŒ Speech Recognition no soportado en este navegador');
      setStatus('error');
      setIsListening(false);
    }
  };





  const handleNext = () => {
    if (conversation.some(turn => turn.isUser) && onNext) {
      onNext();
    }
  };

  const getStatusText = () => {
    switch (status) {
      case 'listening': return 'ğŸ™ï¸ Escuchando...';
      case 'processing': return 'âš™ï¸ Procesando con Gemini...';
      case 'speaking': return 'ğŸ—£ï¸ Hablando...';
      case 'error': return 'âŒ Error de conexiÃ³n';
      default: return 'ğŸ’¬ Listo para conversar';
    }
  };

  // Simplificar: no verificar permisos aquÃ­, Open WebUI los maneja en cada uso

  return (
    <InterfaceContainer>
      {/* Pregunta */}
      <Section>
        <SectionTitle>
          ğŸ“‹ Pregunta
        </SectionTitle>
        <QuestionText>{question}</QuestionText>
      </Section>

      {/* Estado del sistema */}
      <StatusIndicator status={status}>
        {getStatusText()}
      </StatusIndicator>

      {/* Ãrea de conversaciÃ³n */}
      <Section>
        <SectionTitle>
          ğŸ’¬ ConversaciÃ³n
        </SectionTitle>
        
        <ConversationArea>
          {conversation.length === 0 ? (
            <div style={{ 
              textAlign: 'center', 
              color: theme.colors.textLight,
              fontStyle: 'italic',
              padding: theme.spacing.xl
            }}>
              La conversaciÃ³n aparecerÃ¡ aquÃ­...
            </div>
          ) : (
            conversation.map(turn => (
              <ConversationBubble key={turn.id} isUser={turn.isUser}>
                {turn.text}
              </ConversationBubble>
            ))
          )}
        </ConversationArea>
      </Section>

      {/* Estado de conversaciÃ³n */}
      <VoiceControls>
        <StatusIndicator status={status}>
          {getStatusText()}
        </StatusIndicator>
        
        {/* Solo indicador de estado - sin botones manuales */}
      </VoiceControls>

      {/* Botones de acciÃ³n */}
      <ActionButtons>
        {conversation.some(turn => turn.isUser) && onNext && (
          <ActionButton variant="success" onClick={handleNext}>
            âœ… Continuar
          </ActionButton>
        )}
        
        {onSkip && (
          <ActionButton variant="secondary" onClick={onSkip}>
            â­ï¸ Saltar Pregunta
          </ActionButton>
        )}
      </ActionButtons>
    </InterfaceContainer>
  );
};

export default GeminiVoiceInterface;