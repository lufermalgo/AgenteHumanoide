import React, { useState, useEffect, useRef } from 'react';
import styled from 'styled-components';
import { theme } from '../../styles/theme';
import { GeminiLiveService, ASSESSMENT_GEMINI_CONFIG } from '../../services/geminiLive';

const InterfaceContainer = styled.div`
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.lg};
  padding: ${theme.spacing.lg};
  background: ${theme.colors.background};
  border-radius: ${theme.borderRadius.lg};
  max-width: 600px;
  margin: 0 auto;
`;

const Section = styled.div`
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.md};
`;

const SectionTitle = styled.h3`
  font-size: ${theme.typography.fontSizes.lg};
  font-weight: ${theme.typography.fontWeights.semibold};
  color: ${theme.colors.text};
  margin: 0;
  display: flex;
  align-items: center;
  gap: ${theme.spacing.sm};
`;

const QuestionText = styled.div`
  font-size: ${theme.typography.fontSizes.lg};
  color: ${theme.colors.text};
  line-height: 1.6;
  padding: ${theme.spacing.lg};
  background: ${theme.colors.backgroundCard};
  border-radius: ${theme.borderRadius.md};
  border-left: 4px solid ${theme.colors.primary};
`;

const ConversationArea = styled.div`
  min-height: 200px;
  padding: ${theme.spacing.lg};
  background: ${theme.colors.backgroundCard};
  border-radius: ${theme.borderRadius.md};
  border: 2px solid ${theme.colors.lightGray};
  display: flex;
  flex-direction: column;
  gap: ${theme.spacing.md};
`;

const ConversationBubble = styled.div<{ isUser: boolean }>`
  padding: ${theme.spacing.md};
  border-radius: ${theme.borderRadius.md};
  max-width: 80%;
  align-self: ${props => props.isUser ? 'flex-end' : 'flex-start'};
  background: ${props => props.isUser ? theme.colors.primary : theme.colors.lightGray};
  color: ${props => props.isUser ? 'white' : theme.colors.text};
  font-size: ${theme.typography.fontSizes.md};
  line-height: 1.4;
  
  &::before {
    content: '${props => props.isUser ? 'üë§' : 'ü§ñ'}';
    margin-right: ${theme.spacing.sm};
  }
`;

const VoiceControls = styled.div`
  display: flex;
  justify-content: center;
  gap: ${theme.spacing.md};
  align-items: center;
`;

const VoiceButton = styled.button.withConfig({
  shouldForwardProp: (prop) => !['isActive'].includes(prop)
})<{ isActive: boolean; variant?: 'primary' | 'secondary' }>`
  width: 80px;
  height: 80px;
  border-radius: 50%;
  border: 4px solid ${props => {
    if (props.variant === 'secondary') return theme.colors.secondary;
    return props.isActive ? '#ff4444' : theme.colors.primary;
  }};
  background: ${props => {
    if (props.variant === 'secondary') return theme.colors.secondary;
    return props.isActive ? '#ff4444' : theme.colors.primary;
  }};
  color: white;
  font-size: 2rem;
  cursor: pointer;
  transition: all 0.3s ease;
  position: relative;
  overflow: hidden;

  &:hover {
    transform: scale(1.05);
  }

  &:active {
    transform: scale(0.95);
  }

  ${props => props.isActive && `
    animation: pulse 1.5s infinite;
    
    @keyframes pulse {
      0% { box-shadow: 0 0 0 0 rgba(255, 68, 68, 0.7); }
      70% { box-shadow: 0 0 0 20px rgba(255, 68, 68, 0); }
      100% { box-shadow: 0 0 0 0 rgba(255, 68, 68, 0); }
    }
  `}
`;

const StatusIndicator = styled.div<{ status: 'idle' | 'listening' | 'processing' | 'speaking' | 'error' }>`
  display: flex;
  align-items: center;
  gap: ${theme.spacing.sm};
  padding: ${theme.spacing.sm} ${theme.spacing.md};
  border-radius: ${theme.borderRadius.md};
  font-size: ${theme.typography.fontSizes.sm};
  font-weight: ${theme.typography.fontWeights.medium};
  
  background: ${props => {
    switch (props.status) {
      case 'listening': return '#2196F3';
      case 'processing': return '#FF9800';
      case 'speaking': return '#4CAF50';
      case 'error': return '#F44336';
      default: return theme.colors.lightGray;
    }
  }};
  
  color: ${props => props.status === 'idle' ? theme.colors.text : 'white'};
`;

const ActionButtons = styled.div`
  display: flex;
  gap: ${theme.spacing.md};
  justify-content: center;
  flex-wrap: wrap;
`;

const ActionButton = styled.button<{ variant?: 'primary' | 'secondary' | 'success' }>`
  padding: ${theme.spacing.md} ${theme.spacing.lg};
  border: none;
  border-radius: ${theme.borderRadius.md};
  font-size: ${theme.typography.fontSizes.md};
  font-weight: ${theme.typography.fontWeights.medium};
  cursor: pointer;
  transition: all 0.3s ease;
  
  background: ${props => {
    switch (props.variant) {
      case 'success': return '#4CAF50';
      case 'secondary': return theme.colors.lightGray;
      default: return theme.colors.primary;
    }
  }};
  
  color: ${props => props.variant === 'secondary' ? theme.colors.text : 'white'};

  &:hover {
    transform: translateY(-1px);
    background: ${props => {
      switch (props.variant) {
        case 'success': return '#45a049';
        case 'secondary': return theme.colors.tertiary;
        default: return theme.colors.secondary;
      }
    }};
  }

  &:disabled {
    opacity: 0.6;
    cursor: not-allowed;
    transform: none;
  }
`;

interface ConversationTurn {
  id: string;
  isUser: boolean;
  text: string;
  timestamp: Date;
  audioUrl?: string;
}

export interface GeminiVoiceInterfaceProps {
  question: string;
  onResponse: (response: string) => void;
  onNext?: () => void;
  onSkip?: () => void;
  autoReadQuestion?: boolean;
}

const GeminiVoiceInterface: React.FC<GeminiVoiceInterfaceProps> = ({
  question,
  onResponse,
  onNext,
  onSkip,
  autoReadQuestion = true
}) => {
  // Simplificar: no usar hook complejo, manejar stream directamente como Open WebUI
  const [stream, setStream] = useState<MediaStream | null>(null);
  
  const [status, setStatus] = useState<'idle' | 'listening' | 'processing' | 'speaking' | 'error'>('idle');
  const [conversation, setConversation] = useState<ConversationTurn[]>([]);
  const [isListening, setIsListening] = useState(false);
  const [geminiService, setGeminiService] = useState<GeminiLiveService | null>(null);
  const [hasReadQuestion, setHasReadQuestion] = useState(false);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Inicializar Gemini Live API
  useEffect(() => {
    const initGemini = async () => {
      try {
        const service = new GeminiLiveService();
        
        // El servicio ahora funciona de forma s√≠ncrona con la API est√°ndar
        console.log('‚úÖ Servicio Gemini listo para transcripci√≥n');
        
        await service.connect(ASSESSMENT_GEMINI_CONFIG);
        setGeminiService(service);
        console.log('‚úÖ Gemini conectado y listo para usar');
        
      } catch (error) {
        console.error('‚ùå Error inicializando Gemini Live:', error);
        setStatus('error');
      }
    };

    // Inicializar Gemini cuando el componente se monta
    if (!geminiService) {
      initGemini();
    }
    
    return () => {
      geminiService?.disconnect();
    };
  }, [geminiService]);

  // Cargar voces disponibles
  useEffect(() => {
    const loadVoices = () => {
      const voices = speechSynthesis.getVoices();
      if (voices.length > 0) {
        console.log('üó£Ô∏è Voces cargadas:', voices.length);
      }
    };
    
    // Las voces pueden cargarse de forma as√≠ncrona
    if (speechSynthesis.getVoices().length === 0) {
      speechSynthesis.addEventListener('voiceschanged', loadVoices);
    } else {
      loadVoices();
    }
    
    return () => {
      speechSynthesis.removeEventListener('voiceschanged', loadVoices);
    };
  }, []);

  // Auto-leer pregunta
  useEffect(() => {
    if (autoReadQuestion && geminiService?.connected && question && !hasReadQuestion) {
      readQuestion();
      setHasReadQuestion(true);
    }
  }, [autoReadQuestion, geminiService?.connected, question, hasReadQuestion]);

  const addConversationTurn = (isUser: boolean, text: string, audioUrl?: string) => {
    const turn: ConversationTurn = {
      id: Date.now().toString(),
      isUser,
      text,
      timestamp: new Date(),
      audioUrl
    };
    
    setConversation(prev => [...prev, turn]);
    
    if (isUser) {
      onResponse(text);
    }
  };

  const readQuestion = async () => {
    if (!geminiService?.connected) return;
    
    try {
      setStatus('speaking');
      
      // Buscar una voz en espa√±ol m√°s natural
      const voices = speechSynthesis.getVoices();
      console.log('üó£Ô∏è Voces disponibles:', voices.map(v => `${v.name} (${v.lang})`));
      
      // Priorizar voces colombianas, luego espa√±olas, luego cualquier espa√±ol
      const preferredVoice = voices.find(voice => 
        voice.lang.includes('es-CO') || voice.lang.includes('es-MX') || voice.lang.includes('es-AR')
      ) || voices.find(voice => 
        voice.lang.includes('es-ES')
      ) || voices.find(voice => 
        voice.lang.startsWith('es')
      );
      
      const utterance = new SpeechSynthesisUtterance(question);
      utterance.lang = 'es-CO'; // Colombiano si est√° disponible
      utterance.rate = 0.85; // M√°s lento para mejor comprensi√≥n
      utterance.pitch = 1.0; // Tono natural
      utterance.volume = 0.8; // Volumen c√≥modo
      
      if (preferredVoice) {
        utterance.voice = preferredVoice;
        console.log('üó£Ô∏è Usando voz:', preferredVoice.name, preferredVoice.lang);
      }
      
      utterance.onend = () => {
        console.log('üó£Ô∏è Agente termin√≥ de hablar, iniciando escucha autom√°tica...');
        setStatus('idle');
        // Iniciar escucha autom√°tica despu√©s de que termine de hablar
        setTimeout(() => {
          console.log('‚è∞ Timeout ejecutado, iniciando escucha autom√°tica...');
          if (geminiService?.connected) {
            console.log('üéôÔ∏è Iniciando escucha autom√°tica...');
            startListening();
          } else {
            console.warn('‚ö†Ô∏è Gemini no conectado');
          }
        }, 500); // Peque√±a pausa para transici√≥n natural
      };
      utterance.onerror = (error) => {
        console.error('‚ùå Error en s√≠ntesis de voz:', error);
        setStatus('idle');
      };
      
      speechSynthesis.speak(utterance);
    } catch (error) {
      console.error('‚ùå Error leyendo pregunta:', error);
      setStatus('error');
    }
  };

  const startListening = async () => {
    console.log('üéôÔ∏è Iniciando grabaci√≥n siguiendo patr√≥n Open WebUI...');
    console.log('- Gemini conectado:', geminiService?.connected);
    
    if (!geminiService?.connected) {
      console.error('‚ùå Gemini no est√° conectado');
      return;
    }
    
    // Solicitar stream como hace Open WebUI - cada vez que se necesita
    let audioStream: MediaStream | null = null;
    try {
      console.log('üé§ Solicitando stream de audio...');
      audioStream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      if (!audioStream) {
        console.error('‚ùå No se pudo obtener stream');
        return;
      }
      
      console.log('‚úÖ Stream obtenido correctamente');
      setStream(audioStream);
    } catch (err) {
      console.error('‚ùå Error accediendo al micr√≥fono:', err);
      setStatus('error');
      return;
    }
    
    try {
      setIsListening(true);
      setStatus('listening');
      audioChunksRef.current = [];
      
      mediaRecorderRef.current = new MediaRecorder(audioStream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          console.log('üéµ Datos de audio recibidos:', event.data.size, 'bytes');
          audioChunksRef.current.push(event.data);
        }
      };
      
      mediaRecorderRef.current.onstop = async () => {
        console.log('üé§ Grabaci√≥n detenida, procesando audio...');
        console.log('üìä Chunks de audio recolectados:', audioChunksRef.current.length);
        
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm;codecs=opus' });
        const arrayBuffer = await audioBlob.arrayBuffer();
        
        console.log('üìÅ Tama√±o de audio final:', arrayBuffer.byteLength, 'bytes');
        
        if (arrayBuffer.byteLength === 0) {
          console.warn('‚ö†Ô∏è No se captur√≥ audio');
          setStatus('idle');
          setIsListening(false);
          return;
        }
        
        setStatus('processing');
        
        try {
          console.log('üß† Enviando audio a Gemini para transcripci√≥n...');
          // Procesar audio con Gemini AI para transcripci√≥n
          const transcription = await geminiService.processAudio(arrayBuffer, 'audio/webm;codecs=opus');
          
          console.log('‚úÖ Transcripci√≥n recibida:', transcription);
          // Agregar transcripci√≥n a la conversaci√≥n
          addConversationTurn(true, transcription);
          setStatus('idle');
          setIsListening(false); // Cambiar estado aqu√≠ despu√©s de procesar
          
        } catch (error) {
          console.error('‚ùå Error procesando audio:', error);
          setStatus('error');
          setIsListening(false); // Cambiar estado en caso de error
        }
      };
      
      // Agregar onerror para debugging
      mediaRecorderRef.current.onerror = (event) => {
        console.error('‚ùå Error en MediaRecorder:', event);
        setStatus('error');
      };
      
      mediaRecorderRef.current.start(100); // Capturar cada 100ms
      
      // Auto-detener despu√©s de 5 segundos de silencio (simulado por tiempo m√°ximo)
      silenceTimeoutRef.current = setTimeout(() => {
        console.log('üîá Deteniendo grabaci√≥n autom√°ticamente (silencio detectado)');
        console.log('‚è∞ Timeout ejecutado, llamando stopListening...');
        stopListening();
      }, 5000); // 5 segundos m√°ximo de grabaci√≥n continua
      
    } catch (error) {
      console.error('‚ùå Error iniciando grabaci√≥n:', error);
      setStatus('error');
      setIsListening(false);
    }
  };

  const stopListening = () => {
    console.log('‚èπÔ∏è stopListening llamado, estado actual:', {
      mediaRecorder: !!mediaRecorderRef.current,
      isListening,
      mediaRecorderState: mediaRecorderRef.current?.state
    });
    
    if (mediaRecorderRef.current && isListening) {
      console.log('‚èπÔ∏è Deteniendo grabaci√≥n...');
      
      try {
        mediaRecorderRef.current.stop();
        console.log('‚úÖ MediaRecorder.stop() ejecutado');
        // NO cambiar isListening aqu√≠ - dejarlo para que onstop procese
      } catch (error) {
        console.error('‚ùå Error deteniendo MediaRecorder:', error);
        setIsListening(false); // Solo cambiar si hay error
      }
      
      // Limpiar timeout de silencio
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
        silenceTimeoutRef.current = null;
      }
      
      // Parar stream como hace Open WebUI
      if (stream) {
        console.log('üîá Parando tracks del stream...');
        const tracks = stream.getTracks();
        tracks.forEach((track) => track.stop());
        setStream(null);
      }
    } else {
      console.log('‚ö†Ô∏è No se puede detener: MediaRecorder no disponible o no est√° grabando');
    }
  };

  const handleVoiceToggle = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  const handleNext = () => {
    if (conversation.some(turn => turn.isUser) && onNext) {
      onNext();
    }
  };

  const getStatusText = () => {
    switch (status) {
      case 'listening': return 'üéôÔ∏è Escuchando...';
      case 'processing': return '‚öôÔ∏è Procesando con Gemini...';
      case 'speaking': return 'üó£Ô∏è Hablando...';
      case 'error': return '‚ùå Error de conexi√≥n';
      default: return 'üí¨ Listo para conversar';
    }
  };

  // Simplificar: no verificar permisos aqu√≠, Open WebUI los maneja en cada uso

  return (
    <InterfaceContainer>
      {/* Pregunta */}
      <Section>
        <SectionTitle>
          üìã Pregunta
        </SectionTitle>
        <QuestionText>{question}</QuestionText>
      </Section>

      {/* Estado del sistema */}
      <StatusIndicator status={status}>
        {getStatusText()}
      </StatusIndicator>

      {/* √Årea de conversaci√≥n */}
      <Section>
        <SectionTitle>
          üí¨ Conversaci√≥n
        </SectionTitle>
        
        <ConversationArea>
          {conversation.length === 0 ? (
            <div style={{ 
              textAlign: 'center', 
              color: theme.colors.textLight,
              fontStyle: 'italic',
              padding: theme.spacing.xl
            }}>
              La conversaci√≥n aparecer√° aqu√≠...
            </div>
          ) : (
            conversation.map(turn => (
              <ConversationBubble key={turn.id} isUser={turn.isUser}>
                {turn.text}
              </ConversationBubble>
            ))
          )}
        </ConversationArea>
      </Section>

      {/* Estado de conversaci√≥n */}
      <VoiceControls>
        <StatusIndicator status={status}>
          {getStatusText()}
        </StatusIndicator>
        
        {/* Botones de control */}
        {status === 'idle' && !isListening && conversation.length === 0 && (
          <VoiceButton
            isActive={false}
            variant="secondary"
            onClick={handleVoiceToggle}
            title="Iniciar conversaci√≥n manualmente"
          >
            üéôÔ∏è Hablar
          </VoiceButton>
        )}
        
        {isListening && (
          <VoiceButton
            isActive={true}
            variant="secondary"
            onClick={stopListening}
            title="Detener grabaci√≥n manualmente"
          >
            ‚èπÔ∏è Detener
          </VoiceButton>
        )}
      </VoiceControls>

      {/* Botones de acci√≥n */}
      <ActionButtons>
        {conversation.some(turn => turn.isUser) && onNext && (
          <ActionButton variant="success" onClick={handleNext}>
            ‚úÖ Continuar
          </ActionButton>
        )}
        
        {onSkip && (
          <ActionButton variant="secondary" onClick={onSkip}>
            ‚è≠Ô∏è Saltar Pregunta
          </ActionButton>
        )}
      </ActionButtons>
    </InterfaceContainer>
  );
};

export default GeminiVoiceInterface;